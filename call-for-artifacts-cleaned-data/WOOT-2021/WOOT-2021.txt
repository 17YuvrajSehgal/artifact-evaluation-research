WOOT '21 Artifact Evaluation
Evaluation
A scientific paper consists of a constellation of artifacts that extend beyond the document itself: software, hardware, evaluation data and documentation, raw survey results, mechanized proofs, models, test suites, benchmarks, and so on. In some cases, the quality of these artifacts is as important as that of the document itself, yet many of our conferences offer no formal means to submit and evaluate anything but the paper itself. To address this shortcoming, WOOT will run for the first time an optional artifact evaluation process, inspired by similar efforts in software engineering and other areas of science.

Important Dates
All deadlines are 23:59 AoE (Anywhere on Earth):

March 1: Invitation to authors of accepted papers to submit artifacts
March 12: Artifact submission deadline
March 13-March 26: Authors must be reachable for questions in this period
March 27: Notification
Submission
Authors are expected to submit the following:

A PDF with an abstract for the artifact, which specifies the core idea, the focus of the artefact, and what the evaluation should check
A PDF of the most recent version of the accepted paper
Documentation about the artifact (how to reproduce the contributions of the paper)
A link to the artifact, which must be available anonymously (artifact evaluation is single-blind)
Please submit your artifacts to woot21-artifact-submit@vusec.net. Do not include any binary programs in attachments, but link to them where needed.

Overview
A scientific paper consists of a constellation of artifacts that extend beyond the document itself: software, hardware, evaluation data and documentation, raw survey results, mechanized proofs, models, test suites, benchmarks, and so on. In some cases, the quality of these artifacts is as important as that of the document itself, yet many of our conferences offer no formal means to submit and evaluate anything but the paper itself. To address this shortcoming, WOOT will run an optional artifact evaluation process, inspired by similar efforts in software engineering and security conferences.

Criteria
The AEC evaluates whether the artifact does or does not conform to the expectations set by the paper. We expect artifacts to be:

consistent with the paper
as complete as possible
documented well
easy to reuse, facilitating further research
Benefits
We believe the dissemination of artifacts benefits our science and engineering as a whole, as well as the authors submitting them. Their availability improves replicability and reproducibility and enables authors to build on top of each other's work. It can also help more unambiguously resolve questions about cases not considered by the original authors. The authors receive recognition, leading to higher-impact papers, and also benefit themselves from making code reusable.

Process
Artifact evaluation is a separate process from paper reviews, and authors will be asked to submit their artifacts only after their papers have been (conditionally) accepted for publication at WOOT.

After artifact submission, at least one member of the AEC will download and install the artifact (where relevant) and evaluate it. Since we anticipate small glitches with installation and use, reviewers may communicate with authors to help resolve glitches while preserving reviewer anonymity. The AEC will complete its evaluation and notify authors of the outcome.

For the camera ready version, authors that have successfully passed the evaluation process will receive dedicated badges on their papers to demonstrate that their paper has passed this additional evaluation. We also ask the authors to make the artifacts available such that others can replicate the results.

Artifact Details
To avoid excluding some papers, the AEC will try to accept any artifact that authors wish to submit. These can be software, hardware, data sets, survey results, test suites, mechanized proofs, and so on. Given the experience in other communities, we decided to not accept paper proofs in the artifact evaluation process. The AEC lacks the time and often the expertise to carefully review paper proofs. Obviously, the better the artifact is packaged, the more likely the AEC can actually work with it during the evaluation process.

While we encourage open research, submission of an artifact does not contain tacit permission to make its content public. All AEC members will be instructed that they may not publicize any part of your artifact during or after completing the evaluation, nor retain any part of it after evaluation. Thus, you are free to include, e.g., models, data files, or proprietary binaries in your artifact. Also, note that participating in the AEC experiment does not require you to later publish your artifacts, but of course we strongly encourage you to do so.

We recognize that some artifacts may attempt to perform malicious operations by design. These cases should be boldly and explicitly flagged in detail in the readme so AEC members can take appropriate precautions before installing and running these artifacts. The evaluation of exploits and similar results might lead to additional hurdles where we still need to collect experience how to handle this best. Please contact us in case you have concerns, for example when evaluating bug finding tools or other types of artifacts that need special requirements.

AEC Membership
The AEC will consist of about 5–10 members. We intend for other members to be a combination of senior graduate students, postdocs, and researchers. We seek to include a broad cross-section of the WOOT community on the AEC.

If you are interested in joining the AEC, or supervise PhD students who might be, please contact us at woot21-aec@vusec.net.

Committee
Program co-chairs
Mathias Payer (EPFL)
Fangfei Liu (Intel)
Publicity chair
Daniel Gruss (TU Graz)
Artifact evaluation committee
Chair: Erik van der Kouwe (VU)
Ateeq Sharfuddin (SCYTHE)
Brian Chapman (SCYTHE)
Daniel Uroz (University of Zaragoza)
Mohsen Ahmadi (Arizona State University)
Ricardo J. Rodríguez (University of Zaragoza)
Victor Duta (Vrije Universiteit Amsterdam)
Program committee
Johanna Amann, (International Computer Science Institute)
Daniele Antonioli, (EPFL)
Cornelius Aschermann, (Facebook)
Jean-Philippe Aumasson, (Taurus Group)
Dana Baril, (Microsoft)
Lejla Batina, (Radboud University, The Netherlands)
Sarani Bhattacharya, (imec-COSIC, ESAT, KU Leuven)
Kevin Borgolte, (TU Delft)
Juan Caballero, (IMDEA Software Institute)
Yueqiang Cheng, (NIO Security Research)
Chitchanok Chuengsatiansup, (The University of Adelaide, Australia)
Jiska Classen, (TU Darmstadt, Secure Mobile Networking Lab)
Lucas Davi, (University of Duisburg-Essen)
Jennifer Fernick, (NCC Group)
Andrea Fioraldi, (EURECOM)
Yanick Fratantonio, (CISCO Talos)
Christina Garman, (Purdue)
Alexandre Gazet, (Airbus)
Mariano Graziano, (Cisco Talos)
Daniel Gruss, (Graz University of Technology)
Christophe Hauser, (Information Sciences Institute, University of Southern California)
Sean Heelan, (Optimyze)
Rich Johnson, (Fuzzing IO)
Marina Krotofil, (Hamburg University of Technology)
Anil Kurmus, (IBM Research Europe)
Pierre Laperdrix, (CNRS, University of Lille, Inria)
Martina Lindorfer, (TU Wien)
Matt Miller, (Microsoft)
Veelasha Moonsamy, (Ruhr University Bochum)
Asuka Nakajima, (NTT Secure Platform Laboratories)
Yossi Oren, (Ben Gurion University of the Negev, Israel)
Sara Rampazzi, (University of FLorida)
Eyal Ronen, (Tel Aviv University)
Christian Rossow, (CISPA Helmholtz Center for Information Security)
Michael Schwarz, (CISPA Helmholtz Center for Information Security)
Kostya Serebryany, (Google)
Natalie Silvanovich, (Google)
Maddie Stone, (Google Project Zero)
Thomas Unterluggauer, (Intel Labs)
Gabrielle Viala, (Quarkslab)
Lukas Weichselbaum, (Google)
Wenyuan Xu, (Zhejiang University)
Stefano Zanero, (Politecnico di Milano)
Steering committee
Aurélien Francillon, EURECOM
Dan Boneh, Stanford
Yuval Yarom, University of Adelaide and Data61
Clémentine Maurice, CNRS
Sarah Zennou, Airbus
Collin Mulliner, Cruise
Michael Bailey, University of Illinois, Urbana-Champaign